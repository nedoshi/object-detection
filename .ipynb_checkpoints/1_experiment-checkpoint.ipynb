{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "Data exploration and understanding the task at hand is a fundamental step in the Machine Learning workflow.\n",
    "In this notebook, we'll take an opportunity to explore the use case, data and models we'll be using.\n",
    "\n",
    "We have been tasked with developing an application which can identify objects in static and live images. In this notebook we use a pre-trained machine learning model, and explore how it works on static photos. \n",
    "\n",
    "To begin, we import some required python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opencv-python-headless\n",
    "!pip install -q onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nerav/Documents/TL2Demo/object-detection/1_experiment.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nerav/Documents/TL2Demo/object-detection/1_experiment.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m./utils\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nerav/Documents/TL2Demo/object-detection/1_experiment.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m \u001b[39mimport\u001b[39;00m environ\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nerav/Documents/TL2Demo/object-detection/1_experiment.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m InferenceSession\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nerav/Documents/TL2Demo/object-detection/1_experiment.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, ImageColor, ImageDraw, ImageFont\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nerav/Documents/TL2Demo/object-detection/1_experiment.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m coco_classes\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "\n",
    "from os import environ\n",
    "\n",
    "from onnxruntime import InferenceSession\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "from utils.classes import coco_classes\n",
    "from utils.images import preprocess, postprocess, draw_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our image\n",
    "\n",
    "In the next cell we import the image we want to test our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = 'images/redhat-dog.jpg'\n",
    "sample = Image.open(sample_image)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows a visual scene. We need to import the image as an array so the ONNX model we will use can process the image.  If you're curious about the preprocessing code, you can take a look at the [source code](utils/images.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converted_image, scale_factor, padding = preprocess(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "The model we are going to use today is the Tiny YOLO v5 model, which we've included in this repo.  Often you'll download other versions of the model.  We'll be using the [ONNX (Open Neural Network Exchange)](https://onnx.ai/) version of the model, so we can easily see how model inference equates to served models later.  ONNX models are readily served by most Serving Runtimes, including [OpenVINO model server](https://docs.openvino.ai/2023.0/ovms_what_is_openvino_model_server.html) which is included in RHODS by default.\n",
    "\n",
    "You can convert models from most popular frameworks including PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR=\"models/yolov5n\"\n",
    "MODEL_PATH=f\"{MODEL_DIR}/model.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = InferenceSession(MODEL_PATH)\n",
    "raw_result = session.run(\n",
    "    [], {'images': converted_image}\n",
    ")[0]\n",
    "raw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = coco_classes\n",
    "result = postprocess(raw_result, class_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has returned arrays, each of which holds information about the detected objects. The information includes identifiers for the types of objects, coordinates locating the objects within the image, and detection scores, corresponding to how certain the model is about its prediction.\n",
    "\n",
    "We can use a few functions to help us to superimpose the information in this dictionary onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(sample_image, *result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! So you've seen how we can use a pre-trained model to identify objects in images. In the next notebooks, we will deploy this model using RHODS Model Serving, which allows us to use it as part of a larger application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
